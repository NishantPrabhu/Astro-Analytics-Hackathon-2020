{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Techsoc Astro-Analytics Hackathon 2020**\n",
    "### Notebook by **Nishant Prabhu** (Team MechBoisDoingAnalytics)\n",
    "\n",
    "In this notebook, I have described my approach to predicted trajectories and velocities of satellites given simulation and timestamp data. Using the model below, my public leaderboard position was 10 (SMAPE: 28.4637) and private leaderboard position was 11 (SMAPE: 35.3282). Of all the models tried, the one shown belowed gave best results. Details of other attempts have been described in an appropriate section below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training and testing Data**\n",
    "The data given to us was structured as follows:\n",
    "1. `train.csv`: Consists of position and velocity of 600 unique satellites in cartesian system, recorded at different times (timestamp provided). There are 6 simulation columns (3 for position, 3 for velocity) and 6 actual data columns (all of which are supposed to be predicted for records in test data.\n",
    "2. `test.csv`: Consists only of simulated position and velocity data for 300 unique satellites (all of which are present in training data) recorded at different times. For all satellites, testing data starts at a time later than the end of training data (gaps present for some satellites). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('.././mod_data/train_lag2.csv', parse_dates=['epoch'])\n",
    "test = pd.read_csv('.././mod_data/test_lag2.csv', parse_dates=['epoch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feature Engineering**\n",
    "Features related to spherical coordinate system for each satellite were generated using cartesian coordinates through suitable transformations. Lag features (2 time steps before and after each) were generated (see `LagFeaturesGenerator.ipynb`) as we have used tree based models (which assume no interdependence between individual records). Also, we generated second-degree polynomial features of simulated data using `PolynomialFeatures` from `scikit-learn`. Some other features that were tested (but were not useful for the model) were:\n",
    "1. Error between simulated and actual data with time. This was regressed upon first for the testing data, and then this was used as a feature along with simulated data for predicting actual data.\n",
    "2. Hour, day and month of recording using timestamp information, in an attempt to capture seasonal variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_features(train):\n",
    "    train['r_sim'] = np.sqrt(train['x_sim']**2 + train['y_sim']**2 + train['z_sim']**2)\n",
    "    train['xy_r_sim'] = np.sqrt(train['x_sim']**2 + train['y_sim']**2)\n",
    "    train['yz_r_sim'] = np.sqrt(train['y_sim']**2 + train['z_sim']**2)\n",
    "    train['zx_r_sim'] = np.sqrt(train['x_sim']**2 + train['z_sim']**2)\n",
    "    train['alpha_sim'] = np.arccos(train['x_sim']/train['r_sim'])\n",
    "    train['beta_sim'] = np.arccos(train['y_sim']/train['r_sim'])\n",
    "    train['gamma_sim'] = np.arccos(train['z_sim']/train['r_sim'])\n",
    "    train['phi_xy_sim'] = np.arctan(train['x_sim']/train['y_sim'])\n",
    "    train['phi_yz_sim'] = np.arctan(train['y_sim']/train['z_sim'])\n",
    "    train['phi_zx_sim'] = np.arctan(train['z_sim']/train['x_sim'])\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = add_new_features(train)\n",
    "test = add_new_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cols = ['x', 'y', 'z', 'Vx', 'Vy', 'Vz']\n",
    "train_cols = train.drop(['id', 'epoch', 'sat_id'] + pred_cols, axis=1).columns.tolist()\n",
    "lag_cols = [col for col in train_cols if ('_b' in col) or ('_f' in col)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Building and Training**\n",
    "We have used `ExtraTreesRegressor` from `scikit-learn` (500 estimators, random state = 123) to generate our regressor. Other models that we tried (in descending order of performance) included:\n",
    "1. XGBoost Regressor `xgboost.XGBRegressor()`\n",
    "2. K Nearest Neighbors Regressor `sklearn.neighbors.KNeighborsRegressor()`\n",
    "3. LightGBM Regressor `lightgbm.LGBMRegressor()`\n",
    "4. CatBoost Regressor `catboost.CatBoostRegressor()`\n",
    "5. ARIMA (did not tune much, but top performers seem to have used some variant of this) `statsmodels.tsa.arima_model.ARIMA`\n",
    "6. Support Vector Regressor `sklearn.svm.SVR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models \n",
    "reg = ExtraTreesRegressor(n_estimators=500, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing 300 of 300 IDs\n"
     ]
    }
   ],
   "source": [
    "poly_cols = ['x_sim', 'y_sim', 'z_sim', 'Vx_sim', 'Vy_sim', 'Vz_sim']\n",
    "pred_cols = ['x', 'y', 'z', 'Vx', 'Vy', 'Vz']\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "# Main algorithm\n",
    "for i in range(test['sat_id'].nunique()):\n",
    "    \n",
    "    clear_output()\n",
    "    print(\"Now processing {} of {} IDs\".format(i+1, test['sat_id'].nunique()))\n",
    "    idx = test['sat_id'].unique()[i]\n",
    "    \n",
    "    # Extract data for that satellite\n",
    "    d_train = train[train['sat_id'] == idx].reset_index()\n",
    "    d_test = test[test['sat_id'] == idx].reset_index()\n",
    "    \n",
    "    # Training data for position\n",
    "    for cdn in pred_cols:\n",
    "        \n",
    "        X_train, y_train = np.hstack((poly.fit_transform(d_train[poly_cols]), d_train[lag_cols].values)), d_train[cdn].values\n",
    "        X_test = np.hstack((poly.fit_transform(d_test[poly_cols]), d_test[lag_cols].values))\n",
    "        \n",
    "        reg.fit(X_train, y_train)\n",
    "        test_pred = reg.predict(X_test) \n",
    "        \n",
    "        test.loc[test['sat_id'] == idx, cdn] = test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get appropriate columns and create submission\n",
    "sub = test[['id', 'x', 'y', 'z', 'Vx', 'Vy', 'Vz']]\n",
    "sub.to_csv(\".././submissions/sub18_ETR.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
